{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Development\n",
    "\n",
    "This notebook contains code and documentation for developing the retrieval-augmented generation (RAG) functionality using a custom LLM model. The goal is to integrate the LLM with a retrieval mechanism to enhance the response generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install --upgrade pip\n",
    "%pip install sentence-transformers flagembedding langchain langchain-huggingface transformers bitsandbytes accelerate huggingface_hub langchain-chroma langchain-community wikipedia --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Login\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Option 1: Use environment variable (recommended)\n",
    "token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "if token:\n",
    "    login(token=token)\n",
    "    print(\"Logged in using environment variable\")\n",
    "else:\n",
    "    # Option 2: Manual login (uncomment and add your token)\n",
    "    # login(token='your_huggingface_token_here')\n",
    "    print('Please set HUGGINGFACE_TOKEN environment variable or manually add your token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLM Model\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = 'Qwen/Qwen2.5-1.5B-Instruct'  # Model address\n",
    "\n",
    "# Quantization configuration if needed\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype='auto',\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Pipeline and Chat Model\n",
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "\n",
    "# Generation configuration\n",
    "gen_config = dict(\n",
    "    do_sample=True,\n",
    "    max_new_tokens=512,\n",
    "    repetition_penalty=1.1,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    top_k=20\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    **gen_config\n",
    ")\n",
    "\n",
    "# Create LangChain components\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "chat_model = ChatHuggingFace(llm=llm, tokenizer=tokenizer)\n",
    "\n",
    "print(\"Pipeline and chat model setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Embedding Model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import gc\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "\n",
    "# Load and save embedding model\n",
    "emb_model = SentenceTransformer(model_name, device='cpu')\n",
    "emb_model.save('./bge-m3')\n",
    "del emb_model\n",
    "gc.collect()\n",
    "\n",
    "# Load embeddings for LangChain\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name='./bge-m3',\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "print(\"Embedding model setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Documents and Create Vector Database\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "import uuid\n",
    "\n",
    "# Load documents from Wikipedia\n",
    "examples = ['챗GPT', '인공지능', '트랜스포머_(기계_학습)']\n",
    "docs = []\n",
    "for query in examples:\n",
    "    loader = WikipediaLoader(query=query, lang='ko', load_max_docs=1, doc_content_chars_max=1000)\n",
    "    docs += loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=80)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "\n",
    "# Create vector database\n",
    "random_dir = f\"./RAG_db_{str(uuid.uuid4())[:8]}\"\n",
    "print(f\"Creating vector database in: {random_dir}\")\n",
    "\n",
    "db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=random_dir,\n",
    "    collection_metadata={'hnsw:space': 'l2'}\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = db.as_retriever(search_kwargs={'k': 2})\n",
    "\n",
    "print(\"Vector database and retriever setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1"
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define the function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n---\\n\".join('주제: ' + doc.metadata['title'] + '\\n' + doc.page_content for doc in docs)\n",
    "\n",
    "# Initialize the RAG prompt\n",
    "RAG_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Answer the following Question using the Context.'),\n",
    "    ('user', 'Context: {context}\\n---\\nQuestion: {question}')\n",
    "])\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | RAG_prompt\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chain setup complete!\")\n",
    "\n",
    "# Test the RAG chain\n",
    "try:\n",
    "    response = rag_chain.invoke(\"트랜스포머가 뭐예요?\")\n",
    "    print(f\"Response: {response}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Examples\n",
    "\n",
    "You can test the RAG functionality with various questions to see how well the model retrieves and generates responses based on the context provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2"
   },
   "outputs": [],
   "source": [
    "# Test with multiple questions\n",
    "questions = [\n",
    "    \"인공지능의 위험은 없나요?\",\n",
    "    \"챗GPT는 무엇인가요?\",\n",
    "    \"트랜스포머 모델의 특징은?\",\n",
    "    \"인공지능은 어떤 분야인가요?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    try:\n",
    "        print(f\"Question: {question}\")\n",
    "        response = rag_chain.invoke(question)\n",
    "        print(f\"Answer: {response}\\n\")\n",
    "        print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error for question '{question}': {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced RAG with Source Information\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "def format_docs_with_source(docs):\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        formatted.append(f\"[출처 {i}] 주제: {doc.metadata['title']}\\n내용: {doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "# RAG chain that includes source documents\n",
    "rag_chain_from_docs = (\n",
    "    RAG_prompt\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever | format_docs_with_source, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)\n",
    "\n",
    "# Test with source information\n",
    "question = \"인공지능은 어떤 분야인가요?\"\n",
    "try:\n",
    "    result = rag_chain_with_source.invoke(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Context: {result['context']}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging and Troubleshooting\n",
    "print(\"=== System Information ===\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"\\n=== Model Information ===\")\n",
    "print(f\"Model ID: {model_id}\")\n",
    "print(f\"Model device: {model.device}\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "\n",
    "print(\"\\n=== Database Information ===\")\n",
    "print(f\"Vector database directory: {random_dir}\")\n",
    "print(f\"Number of documents in database: {db._collection.count()}\")\n",
    "\n",
    "print(\"\\n=== Testing Components ===\")\n",
    "# Test retriever\n",
    "test_query = \"인공지능\"\n",
    "try:\n",
    "    retrieved_docs = retriever.invoke(test_query)\n",
    "    print(f\"Retrieved {len(retrieved_docs)} documents for query: '{test_query}'\")\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        print(f\"  Doc {i+1}: {doc.metadata['title'][:50]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Retriever error: {e}\")\n",
    "\n",
    "# Test chat model\n",
    "try:\n",
    "    test_response = chat_model.invoke([(\"user\", \"안녕하세요\")])\n",
    "    print(f\"Chat model test successful: {test_response.content[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Chat model error: {e}\")\n",
    "\n",
    "print(\"\\n=== Ready for RAG! ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the setup and usage of a retrieval-augmented generation (RAG) system using a custom LLM model. You can further enhance the functionality by integrating more complex retrieval mechanisms or fine-tuning the LLM based on specific datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
